{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mergePredictions",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yzHOITDlSWq",
        "colab_type": "text"
      },
      "source": [
        "#Merge Predictions\n",
        "This script merges two predictions. It has support for average or additative blending. Furtermore it can produce the final submission file for kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYOaY_-UltEF",
        "colab_type": "text"
      },
      "source": [
        "##Import & Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZkwtYE6q6pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "from skimage import io, util, filters\n",
        "from skimage.filters import *\n",
        "import cv2\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIduQuz-lxPG",
        "colab_type": "text"
      },
      "source": [
        "### Mount Data Location\n",
        "Mounts the Gdrive, where the raw prediction output of the models resides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx5IA2qArCNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')\n",
        " \n",
        " \n",
        "PATH = \"/content/drive/My Drive/CIL Project Images\"\n",
        "print(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDrx477cmik9",
        "colab_type": "text"
      },
      "source": [
        "##Load Data\n",
        "Load the satellite images and the predictions of the models that get combined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRsucSV-0tHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "satellite_path =  PATH  + '/dataset/test_images/'\n",
        "print(\"Loading: \" + satellite_path)\n",
        "satellite_images_list = [cv2.imread(file) for file in sorted(\n",
        "glob.glob(satellite_path + '*.png'))]\n",
        "\n",
        "pix2pix_path = PATH  + \"/predictions/predictions_michi/pix2pix_snapshot/\"\n",
        "print(\"Loading: \" + pix2pix_path)\n",
        "pix2pix_preds = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(\n",
        "glob.glob(pix2pix_path + '*.png'))]\n",
        "\n",
        "unet_path =  PATH  + '/predictions/predictions_michi/dani/report_unet_dilated_5x5_augmented_snapshot_sgd/predictions/'\n",
        "print(\"Loading: \" + unet_path)\n",
        "unet_preds = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(\n",
        "glob.glob(unet_path + '*.png'))]\n",
        "\n",
        "print(len(satellite_images_list))\n",
        "print(len(pix2pix_preds))\n",
        "print(len(unet_preds))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39p72RXRou6I",
        "colab_type": "text"
      },
      "source": [
        "##Merge the predictions\n",
        "Does average blending and additative blending and plots the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSrBjOQmrf9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_blend = []\n",
        "avg_blend = []\n",
        "alpha = 0.5\n",
        "beta = (1.0 - alpha)\n",
        "for p2p, unet in zip(pix2pix_preds, unet_preds):\n",
        "  add_merged = cv2.add(p2p, unet)\n",
        "  avg_merged = cv2.addWeighted(p2p, alpha, unet, beta, 0.0)\n",
        "  add_blend.append(add_merged)\n",
        "  avg_blend.append(avg_merged)\n",
        "\n",
        "for idx in range(len(satellite_images_list)):\n",
        "  plt.figure(figsize=(25,25))\n",
        "\n",
        "  display_list = [satellite_images_list[idx],pix2pix_preds[idx],unet_preds[idx],add_blend[idx],avg_blend[idx]]\n",
        " \n",
        "  title = ['Input Image', 'pix2pix', 'UNet', 'Additative Blend', 'Average Blend']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i], fontsize=24)\n",
        "    plt.imshow(display_list[i], cmap=\"gray\" )\n",
        "    plt.axis('off')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLoVtUzgplPY",
        "colab_type": "text"
      },
      "source": [
        "##Write blend to disk\n",
        "This saves the blends to disk and also ouputs the final kaggle submission file. NOTE: Run the cell directly below first, as it includes the helper code to generate the submission file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPiG_4tBCWFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_blend_path = PATH  + \"/predictions/predictions_michi/merged/blend/\"\n",
        "add_blend_path = PATH  + \"/predictions/predictions_michi/merged/add/\"\n",
        "\n",
        "#Get filenames\n",
        "filenames = []\n",
        "for file in glob.glob(pix2pix_path + \"*.png\"):\n",
        "    filenames.append(os.path.basename(file))\n",
        "filenames = sorted(filenames)\n",
        "\n",
        "#write to disk\n",
        "for idx in range(len(satellite_images_list)):\n",
        "  img_avg = avg_blend[idx]\n",
        "  img_add = add_blend[idx]\n",
        "  img_add_path = add_blend_path + str(filenames[idx])\n",
        "  img_avg_path = avg_blend_path + str(filenames[idx])\n",
        "  print(img_add_path)\n",
        "  print(img_avg_path)\n",
        "  cv2.imwrite(img_add_path, img_add)\n",
        "  cv2.imwrite(img_avg_path, img_avg)\n",
        "\n",
        "#submit predictions\n",
        "submit_predictions(filenames, avg_blend_path)\n",
        "submit_predictions(filenames, add_blend_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM7Ve1iwFEqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "foreground_threshold = 0.35 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
        "\n",
        "# assign a label to a patch\n",
        "def patch_to_label(patch):\n",
        "    df = np.mean(patch)\n",
        "    if df > foreground_threshold:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def mask_to_submission_strings(image_filename):\n",
        "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
        "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
        "    im = mpimg.imread(image_filename)\n",
        "    patch_size = 16\n",
        "    for j in range(0, im.shape[1], patch_size):\n",
        "        for i in range(0, im.shape[0], patch_size):\n",
        "            patch = im[i:i + patch_size, j:j + patch_size]\n",
        "            label = patch_to_label(patch)\n",
        "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
        "\n",
        "\n",
        "def masks_to_submission(submission_filename, *image_filenames):\n",
        "    \"\"\"Converts images into a submission file\"\"\"\n",
        "    with open(submission_filename, 'w') as f:\n",
        "        f.write('id,prediction\\n')\n",
        "        for fn in image_filenames[0:]:\n",
        "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn))\n",
        "\n",
        "#### Applies the \"mask_to_submission file that converts the predicted images to our output format \n",
        "#   Output format: Each image is split into patches of 16 x 16 pixels, and then a 0 or 1 label is assigned to it \n",
        "#   based on our predicted pixel-wise label\n",
        "#   The public test score is based on those patch-wise predictions \n",
        "#  ####\n",
        "\n",
        "def submit_predictions(filenames, path, thresholding = False):\n",
        "  # Path(path + \"/results/csv\").mkdir(parents=True, exist_ok=True)\n",
        "  submission_filename = path + 'submission_' + datetime.datetime.now().strftime('%Y-%m-%d_%H:%M') + '.csv'\n",
        "  image_filenames = []\n",
        "  for i in range(0, 94):\n",
        "    number = filenames[i]\n",
        "    filename = path + number #blend or add\n",
        "    # filename = path + number #old\n",
        "    if not os.path.isfile(filename):\n",
        "        print(filename + \" not found\")\n",
        "        continue\n",
        "    image_filenames.append(filename)\n",
        "    \n",
        "  masks_to_submission(submission_filename, *image_filenames)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}